{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import sklearn.model_selection\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    with open(filename) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        data = []\n",
    "        temp = []\n",
    "        i = 0\n",
    "        for row in csv_reader:\n",
    "            if i>0:\n",
    "                if row[2] =='' or row[3]=='' or row[4]=='':\n",
    "                    if temp != []:\n",
    "                        data.append(temp)\n",
    "                    temp = []\n",
    "                    continue\n",
    "                temp.append([np.float64(row[2]),np.float64(row[3]),np.float64(row[4]),np.float32(row[1].split()[1].split(':')[0])])\n",
    "            i +=1\n",
    "        if len(temp)>0:\n",
    "            data.append(temp)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AirDataset_all(Dataset):\n",
    "\n",
    "    def __init__(self, mode ='train'):\n",
    "        self.mode = mode\n",
    "        self.path = [os.path.join(f'dataset/data_train/input', f) for f in os.listdir(f'dataset/data_train/input')]\n",
    "        self.delay = 36\n",
    "        self.future = 24\n",
    "\n",
    "        self.train = []\n",
    "        self.test = []\n",
    "        self.pre_process()\n",
    "    def __len__(self):\n",
    "        if self.mode =='train':\n",
    "            return len(self.train)\n",
    "        else:\n",
    "            return len(self.test)\n",
    "    def pre_process(self):\n",
    "        data = []\n",
    "        for path in self.path:\n",
    "            raw = read_csv(path)\n",
    "            for period in raw:\n",
    "                if len(period)>=(self.delay+self.future):\n",
    "                    for i in range(len(period)-self.delay-self.future+1):\n",
    "                        data.append(np.stack(period[i:i+self.delay+self.future]))\n",
    "                        \n",
    "            train, test = sklearn.model_selection.train_test_split(data, test_size=0.2,shuffle=False)\n",
    "            self.train += train\n",
    "            self.test += test\n",
    "            data = []\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            input = torch.FloatTensor(self.train[idx][0:self.delay,:])\n",
    "            output = torch.FloatTensor([self.train[idx][self.delay:,0]]).T.squeeze(-1)\n",
    "        if self.mode == 'test':\n",
    "            input = torch.FloatTensor(self.test[idx][0:self.delay,:])\n",
    "            output = torch.FloatTensor([self.test[idx][self.delay:,0]]).T.squeeze(-1)\n",
    "        return input, output\n",
    "\n",
    "def get_loader_all():\n",
    "    train_loader  = DataLoader(dataset=AirDataset_all(mode='train'), \n",
    "                               drop_last=True, \n",
    "                               shuffle=True,\n",
    "                               batch_size=16)\n",
    "    \n",
    "    dev_loader  = DataLoader(dataset=AirDataset_all(mode='test'), \n",
    "                             drop_last=True, \n",
    "                             shuffle=False,\n",
    "                             batch_size=16)\n",
    "\n",
    "    return train_loader, dev_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm  = nn.LSTM(4, 64, 2,bidirectional = True, batch_first=True)\n",
    "        self.lstm1  = nn.LSTM(64, 128, 2,bidirectional = True, batch_first=True)\n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.delay = 24\n",
    "        self.future = 24\n",
    "    def forward(self, x):\n",
    "        _, (h, c) = self.lstm(x)\n",
    "        a = h[-1,:,:]\n",
    "        a = a.unsqueeze(1).repeat(1, self.future, 1)\n",
    "        a, (hidden_state, cell_state) = self.lstm1(a)\n",
    "        a = self.fc1(a)\n",
    "        a = self.tanh(a)\n",
    "        a = self.fc2(a)\n",
    "        a = x[:,-self.future:,0]+a[:,:,0]\n",
    "        a = nn.ReLU()(a)\n",
    "        return a\n",
    "    def compute_loss(self, inp, desire):\n",
    "        output = self(inp)\n",
    "        loss = torch.mean(torch.abs((desire-output)/desire))\n",
    "        return loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ckpt_folder():\n",
    "    folder = f'ckpt/predict_24/checkpoints'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    return folder\n",
    "\n",
    "def get_logs_folder():\n",
    "    return get_ckpt_folder().replace('checkpoints', 'logs')\n",
    "\n",
    "def compute_mdape(y, y_hat):\n",
    "    return np.median(torch.abs((y-y_hat)/y))\n",
    "\n",
    "def compute_mape(y, y_hat):\n",
    "    return torch.mean(torch.abs((y-y_hat)/y))\n",
    "\n",
    "def compute_mae(y, y_hat):\n",
    "    return torch.mean(torch.abs(y-y_hat))\n",
    "\n",
    "def compute_rmse(y, y_hat):\n",
    "    return torch.sqrt(torch.mean(torch.pow(y-y_hat,2)))\n",
    "        \n",
    "def compute_r2(y, y_hat):\n",
    "    return 1- (torch.sum(torch.pow(y-y_hat,2)))/(torch.sum(torch.pow(y-torch.mean(y),2)))\n",
    "        \n",
    "def compute_metrics(x, y, y_hat):\n",
    "    # initialize metrics\n",
    "    metrics = {}\n",
    "    # MDAPE\n",
    "    metrics['MDAPE'] = compute_mdape(y, y_hat)\n",
    "    # MAPE\n",
    "    metrics['MAPE'] = compute_mape(y, y_hat, )\n",
    "    # MAE\n",
    "    metrics['MAE'] = compute_mae(y, y_hat, )\n",
    "    # RMSE\n",
    "    metrics['RMSE'] = compute_rmse(y, y_hat, )\n",
    "    # R2\n",
    "    metrics['R2'] = compute_r2(y, y_hat, )\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # clear cache\n",
    "        self.clear_cache()\n",
    "        # get loader\n",
    "        self.train_loader, self.dev_loader = get_loader_all()\n",
    "        # get model\n",
    "        self.model = LSTM().to('cuda:0')\n",
    "        # get optimizer\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4)\n",
    "        # get writer\n",
    "        self.writer = SummaryWriter(get_logs_folder())\n",
    "        # get iteration\n",
    "        self.iteration = 0\n",
    "        # get epoch\n",
    "        self.num_epoch = 5\n",
    "        self.limit_train_batch = -1\n",
    "        self.log_iter = 10\n",
    "        self.eval_iter = 1\n",
    "    def train_step(self, batch, batch_idx):\n",
    "        # extract data\n",
    "        x, y = batch\n",
    "        x = x.to('cuda:0')\n",
    "        y = y.to('cuda:0')\n",
    "        self.optimizer.zero_grad()\n",
    "        # compute loss\n",
    "        loss_dict = {'loss': self.model.compute_loss(x, y)}\n",
    "        # backward and update weight\n",
    "        loss_dict['loss'].backward()\n",
    "        # clip grad norm\n",
    "        # self.clip_grad_norm()\n",
    "        self.optimizer.step()\n",
    "        return loss_dict\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, mode='dev'):\n",
    "        with torch.no_grad():\n",
    "            # extract data\n",
    "            x, y = batch\n",
    "            x = x.to('cuda:0')\n",
    "\n",
    "            # compute loss\n",
    "            y_hat_device = self.model(x)\n",
    "            y_hat = y_hat_device.detach().cpu().numpy()\n",
    "\n",
    "            # compute metrics\n",
    "            metrics = compute_metrics(x, y, y_hat)\n",
    "        cleaned_metrics = {}\n",
    "        for key in metrics:\n",
    "            cleaned_metrics[f'{mode}:{key}'] = metrics[key]\n",
    "        return cleaned_metrics\n",
    "\n",
    "    def limit_train_batch_hook(self, batch_idx):\n",
    "        if self.limit_train_batch > 0:\n",
    "            if batch_idx > self.limit_train_batch:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def limit_val_batch_hook(self, batch_idx):\n",
    "        if self.limit_val_batch > 0:\n",
    "            if batch_idx > self.limit_val_batch:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_checkpoint_path(self):\n",
    "        ckpt_folder = get_ckpt_folder()\n",
    "        ckpt_name = 'predict24'\n",
    "        return os.path.join(ckpt_folder, ckpt_name) + '.ckpt'\n",
    "\n",
    "    def clear_cache(self):\n",
    "        ckpt_folder = get_ckpt_folder()\n",
    "        logs_folder = get_logs_folder()\n",
    "        if self.clear_cache:\n",
    "            os.system(f'rm -rf {ckpt_folder} {logs_folder}')\n",
    "\n",
    "    # def clip_grad_norm(self):\n",
    "    #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "\n",
    "    def write_dev_metric_to_tensorboard(self, epoch, metrics):\n",
    "        # compute average\n",
    "        for key in metrics:\n",
    "            metrics[key] = np.mean(metrics[key])\n",
    "        # display\n",
    "        print('Evaluate epoch:{}: MDAPE={:0.2f} MAPE={:0.2f},  MAE={:0.2f}, RMSE={:0.2f}, R2={:0.2f}' \\\n",
    "            .format(epoch, metrics['dev:MDAPE'], metrics['dev:MAPE'], metrics['dev:MAE'], metrics['dev:RMSE'],metrics['dev:R2']))\n",
    "        # write to tensorboard\n",
    "        self.writer.add_scalars('validation metric', metrics, epoch)\n",
    "\n",
    "    def write_train_metric_to_tensorboard(self, loss_dicts):\n",
    "        for key in loss_dicts:\n",
    "            loss_dicts[key] = np.mean(loss_dicts[key])\n",
    "        self.writer.add_scalars('training metric', loss_dicts, self.iteration)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        # self.epoch = 0\n",
    "        path = self.get_checkpoint_path()\n",
    "        if os.path.exists(path):\n",
    "            checkpoint = torch.load(path)\n",
    "            print('[+] checkpoint loaded:', path)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            # self.epoch = checkpoint['epoch']\n",
    "            self.iteration = checkpoint['iteration']\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        # load checkpoint\n",
    "        print(self.model)\n",
    "        print('Trainable parameters:', sum(p.numel() for p in self.model.parameters() if p.requires_grad))\n",
    "        print('Non-trainable parameters:', sum(p.numel() for p in self.model.parameters() if not p.requires_grad))\n",
    "        self.load_checkpoint()\n",
    "        for epoch in range(1,self.num_epoch):\n",
    "            # train\n",
    "            loss_dicts = None\n",
    "            self.model.train()\n",
    "            with tqdm.tqdm(self.train_loader, unit=\"it\") as pbar:\n",
    "                pbar.set_description(f'Epoch {epoch}')\n",
    "                for batch_idx, batch in enumerate(pbar):\n",
    "\n",
    "                    # perform training step\n",
    "                    loss_dict = self.train_step(batch, batch_idx)\n",
    "                    if loss_dicts is None:\n",
    "                        loss_dicts = {}\n",
    "                        for key in loss_dict:\n",
    "                            loss_dicts[key] = []\n",
    "                    for key in loss_dict:\n",
    "                        loss_dicts[key].append(float(loss_dict[key].detach().cpu()))\n",
    "\n",
    "                    # limit train batch hook\n",
    "                    if self.limit_train_batch_hook(batch_idx):\n",
    "                        break\n",
    "\n",
    "                    # set postfix\n",
    "                    kwargs = {}\n",
    "                    for key in loss_dict:\n",
    "                        kwargs[key] = float(loss_dict[key].detach().cpu())\n",
    "                    pbar.set_postfix(**kwargs)\n",
    "\n",
    "                    # log\n",
    "                    # self.epoch = epoch\n",
    "                    self.iteration += 1\n",
    "                    if self.iteration % self.log_iter == 0:\n",
    "                        self.write_train_metric_to_tensorboard(loss_dicts)\n",
    "                        loss_dicts = None\n",
    "\n",
    "            ##########################################################################################\n",
    "            # evaluate\n",
    "            if (epoch+1) % self.eval_iter == 0:\n",
    "                self.model.eval()\n",
    "                metrics = {}\n",
    "                with tqdm.tqdm(self.dev_loader, unit=\"it\") as pbar:\n",
    "                    pbar.set_description(f'Evaluate epoch - dev {epoch}')\n",
    "                    for batch_idx, batch in enumerate(pbar):\n",
    "                        # validate\n",
    "                        batch_metrics = self.validation_step(batch, batch_idx, mode='dev')\n",
    "                        # accumulate valilation metrics\n",
    "                        for key in batch_metrics:\n",
    "                            if key not in metrics.keys():\n",
    "                                metrics[key] = []\n",
    "                        for key in batch_metrics:\n",
    "                            metrics[key] += [batch_metrics[key]]\n",
    "                        pbar.set_postfix(MDAPE=np.mean(metrics['dev:MDAPE']))\n",
    "                self.write_dev_metric_to_tensorboard(epoch, metrics)\n",
    "\n",
    "            # save checkpoint\n",
    "            self.save_checkpoint(epoch)\n",
    "    def save_checkpoint(self, epoch):\n",
    "        # save checkpoint\n",
    "        torch.save({\n",
    "            'iteration': self.iteration,\n",
    "            # 'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            }, self.get_checkpoint_path())\n",
    "        print('[+] checkpoint saved')\n",
    "\n",
    "        os.system('cp {} {}'.format(self.get_checkpoint_path(), self.get_checkpoint_path().replace('.ckpt', f'_epoch_{epoch}.ckpt')))\n",
    "        print('[+] checkpoint copied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(4, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (lstm1): LSTM(64, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (tanh): Tanh()\n",
      ")\n",
      "Trainable parameters: 745601\n",
      "Non-trainable parameters: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 3387/3387 [01:03<00:00, 53.49it/s, loss=0.298]\n",
      "Evaluate epoch - dev 1: 100%|██████████| 847/847 [00:06<00:00, 134.94it/s, MDAPE=0.473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate epoch:1: MDAPE=0.47 MAPE=0.66,  MAE=23.03, RMSE=28.37, R2=-2.46\n",
      "[+] checkpoint saved\n",
      "[+] checkpoint copied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3387/3387 [01:00<00:00, 55.98it/s, loss=0.226]\n",
      "Evaluate epoch - dev 2: 100%|██████████| 847/847 [00:04<00:00, 202.73it/s, MDAPE=0.452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate epoch:2: MDAPE=0.45 MAPE=0.62,  MAE=22.24, RMSE=27.55, R2=-2.15\n",
      "[+] checkpoint saved\n",
      "[+] checkpoint copied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3387/3387 [00:58<00:00, 57.67it/s, loss=0.237]\n",
      "Evaluate epoch - dev 3: 100%|██████████| 847/847 [00:04<00:00, 195.28it/s, MDAPE=0.453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate epoch:3: MDAPE=0.45 MAPE=0.60,  MAE=21.90, RMSE=27.09, R2=-2.04\n",
      "[+] checkpoint saved\n",
      "[+] checkpoint copied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3387/3387 [00:59<00:00, 56.95it/s, loss=0.347]\n",
      "Evaluate epoch - dev 4: 100%|██████████| 847/847 [00:04<00:00, 200.66it/s, MDAPE=0.441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate epoch:4: MDAPE=0.44 MAPE=0.59,  MAE=21.41, RMSE=26.57, R2=-1.91\n",
      "[+] checkpoint saved\n",
      "[+] checkpoint copied\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
