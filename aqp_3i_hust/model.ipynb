{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import sklearn.model_selection\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/data_train/location_input.csv')\n",
    "lo_mean = df['longitude'].mean()\n",
    "lo_std = df['longitude'].std()\n",
    "la_mean = df['latitude'].mean()\n",
    "la_std = df['latitude'].std()\n",
    "def location(filename):\n",
    "    with open(filename) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        i = 0\n",
    "        station = {}\n",
    "        for row in csv_reader:\n",
    "            if i>0:\n",
    "                station[row[1]] = [(np.float64(row[2])-lo_mean)/lo_std,  (np.float64(row[3])-la_mean)/la_std]\n",
    "            i +=1\n",
    "\n",
    "    return station \n",
    "# def location(filename):\n",
    "#     with open(filename) as csv_file:\n",
    "#         csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "#         i = 0\n",
    "#         station = {}\n",
    "#         for row in csv_reader:\n",
    "#             if i>0:\n",
    "#                 station[row[1]] = [(np.float64(row[2])),  (np.float64(row[3]))]\n",
    "#             i +=1\n",
    "\n",
    "#     return station \n",
    "\n",
    "def read_csv(filename):\n",
    "    with open(filename) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        data = []\n",
    "        temp = []\n",
    "        i = 0\n",
    "        for row in csv_reader:\n",
    "            if i>0:\n",
    "                if row[2] =='' or row[3]=='' or row[4]=='':\n",
    "                    if temp != []:\n",
    "                        data.append(temp)\n",
    "                    temp = []\n",
    "                    continue\n",
    "                temp.append([np.float64(row[2]),np.float64(row[3]),np.float64(row[4]),np.float32(row[1].split()[1].split(':')[0]),np.float64(row[0])])\n",
    "            i +=1\n",
    "        if len(temp)>0:\n",
    "            data.append(temp)\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_out(filename):\n",
    "    with open(filename) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        data = []\n",
    "        temp = []\n",
    "        i = 0\n",
    "        for row in csv_reader:\n",
    "            if i>0:\n",
    "                if row[2] =='':\n",
    "                    if temp != []:\n",
    "                        data.append(temp)\n",
    "                    temp = []\n",
    "                    continue\n",
    "                temp.append([np.float64(row[2]),np.float64(row[0])])\n",
    "            i +=1\n",
    "    return data\n",
    "\n",
    "def pre_process():\n",
    "    path_in = [os.path.join(f'dataset/data_train/input', f) for f in os.listdir(f'dataset/data_train/input')]\n",
    "    path_out = [os.path.join(f'dataset/data_train/output', f) for f in os.listdir(f'dataset/data_train/output')]\n",
    "    input_location = location('dataset/data_train/location_input.csv')\n",
    "    output_location = location('dataset/data_train/location_output.csv')\n",
    "    inp = {}\n",
    "    inp_local = {}\n",
    "    for path1 in path_in:\n",
    "        locate = os.path.basename(path1).split('.')[0]\n",
    "        raw = read_csv(path1)\n",
    "        for period in raw:\n",
    "            if len(period)>=(24+24):\n",
    "                for i in range(len(period)-24-23):\n",
    "                    key  = str(int(period[i][-1]))\n",
    "                    if key not in inp.keys():\n",
    "                        inp[key] = []\n",
    "                        inp_local[key] = []\n",
    "                    inp[key] += [np.stack(period[i:i+24+24])]\n",
    "                    inp_local[key] += [input_location[locate]]\n",
    "                    \n",
    "    out = {}\n",
    "    out_local = {}\n",
    "    for path1 in path_out:\n",
    "        locate = os.path.basename(path1).split('.')[0]\n",
    "        raw = read_out(path1)\n",
    "        for period in raw:\n",
    "            if len(period)>=(24+24):\n",
    "                for i in range(len(period)-24-23):\n",
    "                    key  = str(int(period[i][-1]))\n",
    "                    if key not in out.keys():\n",
    "                        out[key] = []\n",
    "                        out_local[key] = []\n",
    "                    out[key] += [np.stack(period[i+24:i+24+24])]\n",
    "                    out_local[key] += [output_location[locate]]            \n",
    "    data = []\n",
    "    temp = 0\n",
    "    input_temp = 0\n",
    "    input_local_temp = 0\n",
    "    output_temp = 0\n",
    "    output_local_temp = 0\n",
    "    for state in out.keys():\n",
    "        if state in inp.keys():\n",
    "            input_temp = np.stack(inp[state])[:,:,:-1]\n",
    "            input_local_temp = np.stack(inp_local[state])\n",
    "            for station in range(len(out[state])):\n",
    "                output_temp = np.stack(out[state][station])[:,0]\n",
    "                output_local_temp = np.stack(out_local[state][station])         \n",
    "                data += [[input_temp, input_local_temp, output_temp, output_local_temp]]\n",
    "\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AirDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mode ='train'):\n",
    "        self.mode = mode\n",
    "        self.data = []\n",
    "        self.delay = 36\n",
    "        self.future = 24\n",
    "        self.pre_process()\n",
    "        self.train = 0\n",
    "        self.test = 0\n",
    "\n",
    "        self.split_data()\n",
    "        \n",
    "            \n",
    "    def __len__(self):\n",
    "        if self.mode =='train':\n",
    "            return len(self.train)\n",
    "        else:\n",
    "            return len(self.test)\n",
    "    def pre_process(self):\n",
    "        path_in = [os.path.join(f'dataset/data_train/input', f) for f in os.listdir(f'dataset/data_train/input')]\n",
    "        path_out = [os.path.join(f'dataset/data_train/output', f) for f in os.listdir(f'dataset/data_train/output')]\n",
    "        input_location = location('dataset/data_train/location_input.csv')\n",
    "        output_location = location('dataset/data_train/location_output.csv')\n",
    "        inp = {}\n",
    "        inp_local = {}\n",
    "        for path1 in path_in:\n",
    "            locate = os.path.basename(path1).split('.')[0]\n",
    "            raw = read_csv(path1)\n",
    "            for period in raw:\n",
    "                if len(period)>=(self.delay):\n",
    "                    for i in range(len(period)-self.delay+1):\n",
    "                        key  = str(int(period[i][-1])+self.delay)\n",
    "                        if key not in inp.keys():\n",
    "                            inp[key] = []\n",
    "                            inp_local[key] = []\n",
    "                        inp[key] += [np.stack(period[i:i+self.delay])]\n",
    "                        inp_local[key] += [input_location[locate]]\n",
    "                        \n",
    "        out = {}\n",
    "        out_local = {}\n",
    "        for path1 in path_out:\n",
    "            locate = os.path.basename(path1).split('.')[0]\n",
    "            raw = read_out(path1)\n",
    "            for period in raw:\n",
    "                if len(period)>=(self.future):\n",
    "                    for i in range(len(period)-self.future-+1):\n",
    "                        key  = str(int(period[i][-1]))\n",
    "                        if key not in out.keys():\n",
    "                            out[key] = []\n",
    "                            out_local[key] = []\n",
    "                        out[key] += [np.stack(period[i:i++self.future])]\n",
    "                        out_local[key] += [output_location[locate]]            \n",
    "        data = []\n",
    "        temp = 0\n",
    "        input_temp = 0\n",
    "        input_local_temp = 0\n",
    "        output_temp = 0\n",
    "        output_local_temp = 0\n",
    "        for state in out.keys():\n",
    "            if state in inp.keys():\n",
    "                input_temp = np.stack(inp[state])[:,:,:-1]\n",
    "                input_local_temp = np.stack(inp_local[state])\n",
    "                for station in range(len(out[state])):\n",
    "                    output_temp = np.stack(out[state][station])[:,0]\n",
    "                    output_local_temp = np.stack(out_local[state][station])         \n",
    "                    data += [[input_temp, input_local_temp, output_temp, output_local_temp]]\n",
    "        self.data = data\n",
    "    def split_data(self):\n",
    "        random.Random(0).shuffle(self.data)\n",
    "        self.train, self.test = sklearn.model_selection.train_test_split(self.data, test_size=0.2, shuffle=True)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            [input_temp, input_local_temp, output_temp, output_local_temp] = self.train[idx]\n",
    "            input_temp = torch.FloatTensor(input_temp[:,0:self.delay,:])\n",
    "            input_local_temp = torch.FloatTensor(input_local_temp)\n",
    "            output_temp = torch.FloatTensor(output_temp)\n",
    "            output_local_temp = torch.FloatTensor(output_local_temp)\n",
    "        if self.mode == 'test':\n",
    "            [input_temp, input_local_temp, output_temp, output_local_temp] = self.test[idx]\n",
    "            input_temp = torch.FloatTensor(input_temp[:,0:self.delay,:])\n",
    "            input_local_temp = torch.FloatTensor(input_local_temp)\n",
    "            output_temp = torch.FloatTensor(output_temp)\n",
    "            output_local_temp = torch.FloatTensor(output_local_temp)\n",
    "        return input_temp, input_local_temp, output_temp, output_local_temp\n",
    "\n",
    "def get_loader():\n",
    "    train_loader  = DataLoader(dataset=AirDataset(mode='train'), \n",
    "                               drop_last=True, \n",
    "                               shuffle=True,\n",
    "                               batch_size=1)\n",
    "    \n",
    "    dev_loader  = DataLoader(dataset=AirDataset(mode='test'), \n",
    "                             drop_last=True, \n",
    "                             shuffle=False,\n",
    "                             batch_size=1)\n",
    "\n",
    "    return train_loader, dev_loader\n",
    "# train_loader, dev_loader = get_loader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm  = nn.LSTM(4, 64, 2,bidirectional = True, batch_first=True)\n",
    "        self.lstm1  = nn.LSTM(64, 128, 2,bidirectional = True, batch_first=True)\n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.delay = 24\n",
    "        self.future = 24\n",
    "    def forward(self, x):\n",
    "        _, (h, c) = self.lstm(x)\n",
    "        a = h[-1,:,:]\n",
    "        a = a.unsqueeze(1).repeat(1, self.future, 1)\n",
    "        a, (hidden_state, cell_state) = self.lstm1(a)\n",
    "        # a = x.reshape((-1, self.seq_len, self.hidden_size))\n",
    "        a = self.fc1(a)\n",
    "        a = self.tanh(a)\n",
    "        a = self.fc2(a)\n",
    "        a = x[:,-self.future:,0]+a[:,:,0]\n",
    "        a = nn.ReLU()(a)\n",
    "        return a\n",
    "\n",
    "    \n",
    "model = LSTM()\n",
    "checkpoint = torch.load('ckpt/predict_24/checkpoints/predict24_epoch_4.ckpt', map_location='cuda:0')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "class REG(nn.Module):\n",
    "\n",
    "    def __init__(self, bias=True):\n",
    "        super(REG, self).__init__()\n",
    "        self.predict24 = model.requires_grad_(False)\n",
    "        self.adj_transform1 = nn.Linear(4, 10)\n",
    "        self.adj_transform2 = nn.Linear(10, 1)\n",
    "        self.linear1 = nn.Linear(24,100)\n",
    "        self.linear2 = nn.Linear(100,24)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, x, x_l, y_l, return_adj=False):\n",
    "        a = self.predict24(x)\n",
    "        S, _ = a.shape\n",
    "        \n",
    "        # construct adjacency matrix\n",
    "        y_l = y_l.expand([S,2])\n",
    "        adj = torch.cat((x_l, y_l), dim=-1)\n",
    "        \n",
    "        # non-linear function\n",
    "        adj = self.adj_transform1(adj)\n",
    "        adj = self.tanh(adj)\n",
    "        adj = self.adj_transform2(adj)\n",
    "        out = torch.mm(a.transpose(-1,0), adj)\n",
    "        \n",
    "        # activation\n",
    "        out = out.transpose(-1,0)\n",
    "        out = out.relu()\n",
    "        if return_adj:\n",
    "            return out, adj\n",
    "        return out\n",
    "    def compute_loss(self, x, x_l, y_l, desire):\n",
    "        output = self(x, x_l, y_l)\n",
    "        loss = torch.mean(torch.abs((desire-output)/desire))\n",
    "        return loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ckpt_folder():\n",
    "    folder = f'ckpt/model/checkpoints'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    return folder\n",
    "\n",
    "def get_logs_folder():\n",
    "    return get_ckpt_folder().replace('checkpoints', 'logs')\n",
    "\n",
    "def compute_mdape(y, y_hat):\n",
    "    return np.median(torch.abs((y-y_hat)/y))\n",
    "\n",
    "def compute_mape(y, y_hat):\n",
    "    return torch.mean(torch.abs((y-y_hat)/y))\n",
    "\n",
    "def compute_mae(y, y_hat):\n",
    "    return torch.mean(torch.abs(y-y_hat))\n",
    "\n",
    "def compute_rmse(y, y_hat):\n",
    "    return torch.sqrt(torch.mean(torch.pow(y-y_hat,2)))\n",
    "        \n",
    "def compute_r2(y, y_hat):\n",
    "    return 1- (torch.sum(torch.pow(y-y_hat,2)))/(torch.sum(torch.pow(y-torch.mean(y),2)))\n",
    "        \n",
    "def compute_metrics(x, y, y_hat):\n",
    "    # initialize metrics\n",
    "    metrics = {}\n",
    "    # MDAPE\n",
    "    metrics['MDAPE'] = compute_mdape(y, y_hat)\n",
    "    # MAPE\n",
    "    metrics['MAPE'] = compute_mape(y, y_hat, )\n",
    "    # MAE\n",
    "    metrics['MAE'] = compute_mae(y, y_hat, )\n",
    "    # RMSE\n",
    "    metrics['RMSE'] = compute_rmse(y, y_hat, )\n",
    "    # R2\n",
    "    metrics['R2'] = compute_r2(y, y_hat, )\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        # clear cache\n",
    "        self.clear_cache()\n",
    "        # get loader\n",
    "        self.train_loader, self.dev_loader = get_loader()\n",
    "        # get model\n",
    "        self.model = REG().to('cuda:0')\n",
    "        # get optimizer\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4)\n",
    "        # get writer\n",
    "        self.writer = SummaryWriter(get_logs_folder())\n",
    "        # get iteration\n",
    "        self.iteration = 0\n",
    "        # get epoch\n",
    "        self.num_epoch = 10\n",
    "        self.limit_train_batch = 5000\n",
    "        self.log_iter = 100\n",
    "        self.eval_iter = 1\n",
    "    def train_step(self, batch, batch_idx):\n",
    "        # extract data\n",
    "        x, x_l, y, y_l = batch\n",
    "        x = x.to('cuda:0').squeeze(0)\n",
    "        y = y.to('cuda:0').squeeze(0)\n",
    "        x_l = x_l.to('cuda:0').squeeze(0)\n",
    "        y_l = y_l.to('cuda:0').squeeze(0)\n",
    "        self.optimizer.zero_grad()\n",
    "        # compute loss\n",
    "        loss_dict = {'loss': self.model.compute_loss(x, x_l, y_l, y)}\n",
    "        # backward and update weight\n",
    "        loss_dict['loss'].backward()\n",
    "        # clip grad norm\n",
    "        # self.clip_grad_norm()\n",
    "        self.optimizer.step()\n",
    "        return loss_dict\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, mode='dev'):\n",
    "        with torch.no_grad():\n",
    "            # extract data\n",
    "            x, x_l, y, y_l = batch\n",
    "            x = x.to('cuda:0')[0]\n",
    "            x_l = x_l.to('cuda:0')[0]\n",
    "            y_l = y_l.to('cuda:0')[0]\n",
    "            # compute loss\n",
    "            y_hat_device = self.model(x, x_l, y_l)\n",
    "            y_hat = y_hat_device.detach().cpu().numpy()\n",
    "\n",
    "            # compute metrics\n",
    "            metrics = compute_metrics(x, y, y_hat)\n",
    "        cleaned_metrics = {}\n",
    "        for key in metrics:\n",
    "            cleaned_metrics[f'{mode}:{key}'] = metrics[key]\n",
    "        return cleaned_metrics\n",
    "\n",
    "    def limit_train_batch_hook(self, batch_idx):\n",
    "        if self.limit_train_batch > 0:\n",
    "            if batch_idx > self.limit_train_batch:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def limit_val_batch_hook(self, batch_idx):\n",
    "        if self.limit_val_batch > 0:\n",
    "            if batch_idx > self.limit_val_batch:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_checkpoint_path(self):\n",
    "        ckpt_folder = get_ckpt_folder()\n",
    "        ckpt_name = 'mlp'\n",
    "        return os.path.join(ckpt_folder, ckpt_name) + '.ckpt'\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        # self.epoch = 0\n",
    "        path = self.get_checkpoint_path()\n",
    "        if os.path.exists(path):\n",
    "            checkpoint = torch.load(path)\n",
    "            print('[+] checkpoint loaded:', path)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            # self.epoch = checkpoint['epoch']\n",
    "            self.iteration = checkpoint['iteration']\n",
    "\n",
    "    def clear_cache(self):\n",
    "        ckpt_folder = get_ckpt_folder()\n",
    "        logs_folder = get_logs_folder()\n",
    "        if self.clear_cache:\n",
    "            os.system(f'rm -rf {ckpt_folder} {logs_folder}')\n",
    "\n",
    "    def write_dev_metric_to_tensorboard(self, epoch, metrics):\n",
    "        # compute average\n",
    "        for key in metrics:\n",
    "            metrics[key] = np.mean(metrics[key])\n",
    "        # display\n",
    "        print('Evaluate epoch:{}: MDAPE={:0.2f} MAPE={:0.2f},  MAE={:0.2f}, RMSE={:0.2f}, R2={:0.2f}' \\\n",
    "            .format(epoch, metrics['dev:MDAPE'], metrics['dev:MAPE'], metrics['dev:MAE'], metrics['dev:RMSE'],metrics['dev:R2']))\n",
    "        # write to tensorboard\n",
    "        self.writer.add_scalars('validation metric', metrics, epoch)\n",
    "\n",
    "    def write_train_metric_to_tensorboard(self, loss_dicts):\n",
    "        for key in loss_dicts:\n",
    "            loss_dicts[key] = np.mean(loss_dicts[key])\n",
    "        self.writer.add_scalars('training metric', loss_dicts, self.iteration)\n",
    "\n",
    "    def fit(self):\n",
    "        # load checkpoint\n",
    "        print(self.model)\n",
    "        print('Trainable parameters:', sum(p.numel() for p in self.model.parameters() if p.requires_grad))\n",
    "        print('Non-trainable parameters:', sum(p.numel() for p in self.model.parameters() if not p.requires_grad))\n",
    "        self.load_checkpoint()\n",
    "        for epoch in range(1,self.num_epoch):\n",
    "            self.train_loader, self.dev_loader = get_loader()\n",
    "            # train\n",
    "            if epoch > 3:\n",
    "                self.model.predict24.requires_grad_(True)\n",
    "                self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "            loss_dicts = None\n",
    "            self.model.train()\n",
    "            with tqdm.tqdm(self.train_loader, unit=\"it\") as pbar:\n",
    "                pbar.set_description(f'Epoch {epoch}')\n",
    "                for batch_idx, batch in enumerate(pbar):\n",
    "\n",
    "                    # perform training step\n",
    "                    loss_dict = self.train_step(batch, batch_idx)\n",
    "                    if loss_dicts is None:\n",
    "                        loss_dicts = {}\n",
    "                        for key in loss_dict:\n",
    "                            loss_dicts[key] = []\n",
    "                    for key in loss_dict:\n",
    "                        loss_dicts[key].append(float(loss_dict[key].detach().cpu()))\n",
    "\n",
    "                    # limit train batch hook\n",
    "                    if self.limit_train_batch_hook(batch_idx):\n",
    "                        break\n",
    "\n",
    "                    # set postfix\n",
    "                    kwargs = {}\n",
    "                    for key in loss_dict:\n",
    "                        kwargs[key] = float(loss_dict[key].detach().cpu())\n",
    "                    pbar.set_postfix(**kwargs)\n",
    "\n",
    "                    # log\n",
    "                    # self.epoch = epoch\n",
    "                    self.iteration += 1\n",
    "                    if self.iteration % self.log_iter == 0:\n",
    "                        self.write_train_metric_to_tensorboard(loss_dicts)\n",
    "                        loss_dicts = None\n",
    "\n",
    "            ##########################################################################################\n",
    "            # evaluate\n",
    "            if (epoch+1) % self.eval_iter == 0:\n",
    "                self.model.eval()\n",
    "                metrics = {}\n",
    "                with tqdm.tqdm(self.dev_loader, unit=\"it\") as pbar:\n",
    "                    pbar.set_description(f'Evaluate epoch - dev {epoch}')\n",
    "                    for batch_idx, batch in enumerate(pbar):\n",
    "                        # validate\n",
    "                        batch_metrics = self.validation_step(batch, batch_idx, mode='dev')\n",
    "                        # accumulate valilation metrics\n",
    "                        for key in batch_metrics:\n",
    "                            if key not in metrics.keys():\n",
    "                                metrics[key] = []\n",
    "                        for key in batch_metrics:\n",
    "                            metrics[key] += [batch_metrics[key]]\n",
    "                        pbar.set_postfix(MDAPE=np.mean(metrics['dev:MDAPE']))\n",
    "                self.write_dev_metric_to_tensorboard(epoch, metrics)\n",
    "            self.save_checkpoint(epoch)\n",
    "    def save_checkpoint(self, epoch):\n",
    "        # save checkpoint\n",
    "        torch.save({\n",
    "            'iteration': self.iteration,\n",
    "            # 'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            }, self.get_checkpoint_path())\n",
    "        print('[+] checkpoint saved')\n",
    "\n",
    "        os.system('cp {} {}'.format(self.get_checkpoint_path(), self.get_checkpoint_path().replace('.ckpt', f'_epoch_{epoch}.ckpt')))\n",
    "        print('[+] checkpoint copied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REG(\n",
      "  (predict24): LSTM(\n",
      "    (lstm): LSTM(4, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
      "    (lstm1): LSTM(64, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "    (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      "  (adj_transform1): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (adj_transform2): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (linear1): Linear(in_features=24, out_features=100, bias=True)\n",
      "  (linear2): Linear(in_features=100, out_features=24, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (tanh): Tanh()\n",
      ")\n",
      "Trainable parameters: 4985\n",
      "Non-trainable parameters: 745601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  20%|██        | 5001/24724 [00:33<02:13, 148.00it/s, loss=0.195] \n",
      "Evaluate epoch - dev 1: 100%|██████████| 6181/6181 [00:35<00:00, 174.90it/s, MDAPE=0.494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate epoch:1: MDAPE=0.49 MAPE=0.57,  MAE=25.37, RMSE=29.74, R2=-3.21\n",
      "[+] checkpoint saved\n",
      "[+] checkpoint copied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  20%|██        | 5001/24724 [00:29<01:57, 168.42it/s, loss=0.314] \n",
      "Evaluate epoch - dev 2: 100%|██████████| 6181/6181 [00:32<00:00, 187.59it/s, MDAPE=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate epoch:2: MDAPE=0.50 MAPE=0.57,  MAE=26.43, RMSE=30.71, R2=-3.51\n",
      "[+] checkpoint saved\n",
      "[+] checkpoint copied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  20%|██        | 5001/24724 [00:33<02:13, 147.26it/s, loss=0.532] \n",
      "Evaluate epoch - dev 3: 100%|██████████| 6181/6181 [00:33<00:00, 183.94it/s, MDAPE=0.497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate epoch:3: MDAPE=0.50 MAPE=0.57,  MAE=25.68, RMSE=30.01, R2=-3.27\n",
      "[+] checkpoint saved\n",
      "[+] checkpoint copied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  20%|██        | 5001/24724 [01:32<06:05, 54.00it/s, loss=0.275]\n",
      "Evaluate epoch - dev 4: 100%|██████████| 6181/6181 [00:40<00:00, 153.20it/s, MDAPE=0.453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate epoch:4: MDAPE=0.45 MAPE=0.52,  MAE=22.19, RMSE=26.58, R2=-2.78\n",
      "[+] checkpoint saved\n",
      "[+] checkpoint copied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:  20%|██        | 5001/24724 [01:37<06:25, 51.13it/s, loss=0.404] \n",
      "Evaluate epoch - dev 5: 100%|██████████| 6181/6181 [00:39<00:00, 154.97it/s, MDAPE=0.441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate epoch:5: MDAPE=0.44 MAPE=0.51,  MAE=21.77, RMSE=26.23, R2=-2.71\n",
      "[+] checkpoint saved\n",
      "[+] checkpoint copied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:  20%|██        | 5001/24724 [01:38<06:29, 50.64it/s, loss=0.509]\n",
      "Evaluate epoch - dev 6: 100%|██████████| 6181/6181 [00:40<00:00, 153.55it/s, MDAPE=0.439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate epoch:6: MDAPE=0.44 MAPE=0.50,  MAE=21.53, RMSE=25.96, R2=-2.68\n",
      "[+] checkpoint saved\n",
      "[+] checkpoint copied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  20%|██        | 5001/24724 [01:39<06:31, 50.32it/s, loss=0.294] \n",
      "Evaluate epoch - dev 7: 100%|██████████| 6181/6181 [00:40<00:00, 151.33it/s, MDAPE=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate epoch:7: MDAPE=0.44 MAPE=0.50,  MAE=21.74, RMSE=26.10, R2=-2.91\n",
      "[+] checkpoint saved\n",
      "[+] checkpoint copied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8:  20%|██        | 5001/24724 [01:39<06:33, 50.07it/s, loss=0.664] \n",
      "Evaluate epoch - dev 8: 100%|██████████| 6181/6181 [00:40<00:00, 152.81it/s, MDAPE=0.439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate epoch:8: MDAPE=0.44 MAPE=0.49,  MAE=22.13, RMSE=26.52, R2=-2.94\n",
      "[+] checkpoint saved\n",
      "[+] checkpoint copied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:  20%|██        | 5001/24724 [01:39<06:32, 50.30it/s, loss=0.474] \n",
      "Evaluate epoch - dev 9: 100%|██████████| 6181/6181 [00:40<00:00, 151.51it/s, MDAPE=0.431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate epoch:9: MDAPE=0.43 MAPE=0.49,  MAE=21.71, RMSE=26.08, R2=-2.72\n",
      "[+] checkpoint saved\n",
      "[+] checkpoint copied\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard dev upload --logdir /home/dat/aqp/ckpt/logs --name 'lstm-parallel' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('/home/dat/aqp/ckpt/checkpoints/mlp_epoch_19.ckpt')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
